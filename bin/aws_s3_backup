#!/usr/bin/env -S bash -e

command_name=$(basename "$0") && readonly command_name
current_dir=$(pwd) && readonly current_dir
timestamp=$(date +%Y%m%d_%H%M%S) && readonly timestamp

purge=0
bucket=
dst_tmp_sync_dir=
dst_root_dir=
log_file=

has() { command -v "${1}" >&/dev/null; }
_ink() { if has ink; then ink "$@"; else cat - 1>&2; fi; }
# shellcheck disable=SC2145
_log() { echo "$(date +"%Y-%m-%d %H:%M:%S") ${@:2}" | _ink "$1"; }
log() { _log white "$*"; }
info() { _log cyan "$*"; }
warn() { _log yellow "$*"; }
error() { _log red "$*" && exit 1; }

usage() {
  cat <<EOF

Backup s3 bucket to local directory

  Usage:
      $command_name [option]
    Options
      -h|--help  : Show this usage
      -p|--purge : Delete object that backupped from s3
      -d|--dst   : Specify export directory

EOF
}

initialize() {
  while true; do
    [[ -z $1 ]] && break
    case "$1" in
      -h | --help) usage && exit 0 ;;
      -p | --purge) purge=1 ;;
      -d | --dst) shift && dst_root_dir="$1" ;;
      *) bucket="$1" ;;
    esac
    shift
  done
  if [[ -z $dst_root_dir ]]; then
    dst_root_dir="$current_dir/${command_name}_logs"
  fi
  check

  log_dir="$dst_root_dir/log"
  mkdir -p "$log_dir"
  log_file="$dst_root_dir/${timestamp}_${bucket}.log"
  set -eu
}

check() {
  if [[ -z $bucket ]]; then
    error "==> Specify bucket name"
  fi
}

end() {
  if [[ $? -eq 0 ]]; then
    mv "$log_file" "$log_dir/"
  fi
  warn "==> Removing $dst_tmp_sync_dir.."
  rm -rf "$dst_tmp_sync_dir"
  log "==> Done"
}

download_bucket() {
  info "==> Downloading from s3://$bucket to $dst_tmp_sync_dir.."
  # aws s3 sync "s3://$bucket" "$dst_tmp_sync_dir" || info "==> Failed to aws s3 sync.."
  aws s3 cp --recursive "s3://$bucket" "$dst_tmp_sync_dir" || info "==> Failed to aws s3 sync.."
  info "==> Downloaded  from s3://$bucket to $dst_tmp_sync_dir."
}

copy_to_persistence() {
  info "==> Copying from $dst_tmp_sync_dir to $dst_bucket_dir.."
  (
    find "$dst_tmp_sync_dir/" -type f -print -quit | grep -q . || {
      info "==> No files to copy from $dst_tmp_sync_dir."
      return 0
    }
    cd "$dst_tmp_sync_dir/" && find ./ -type f -print0 |
      xargs -0 cp --parents -vft "$dst_bucket_dir"
  )
  info "==>  Copied from $dst_tmp_sync_dir to $dst_bucket_dir."
}

purge_synced_files_from_s3() {
  info "==> Starting purge process for synced files..."

  local s3_files_list="$dst_tmp_sync_dir/s3_files.txt"
  local local_files_list="$dst_tmp_sync_dir/local_files.txt"
  local files_to_delete="$dst_tmp_sync_dir/files_to_delete.txt"

  info "==> Fetching S3 file list from s3://$bucket..."
  aws s3 ls "s3://$bucket/" --recursive |
    # Remove the first three columns (date, time, size)
    # ex)
    #   2024-06-01 12:34:56 12345 path/to/file.txt
    #   2024-06-01 12:35:00 67890 another/file.log
    awk '{$1=$2=$3=""; print $0}' |
    sed 's/^ *//' |
    sort >"$s3_files_list"

  local s3_file_count
  s3_file_count=$(wc -l <"$s3_files_list")
  info "==> Found $s3_file_count files in S3"

  info "==> Creating local file list from $dst_bucket_dir..."
  find "$dst_bucket_dir" -type f |
    sed "s|^$dst_bucket_dir/||" |
    sort >"$local_files_list"

  local local_file_count
  local_file_count=$(wc -l <"$local_files_list")
  info "==> Found $local_file_count files locally"

  info "==> Comparing file lists..."
  # comm: commコマンドは、2つのソートされたファイルを比較し、共通部分や差分を表示します。
  #   -1: 1番目のファイルにのみ存在する行を表示しない
  #   -2: 2番目のファイルにのみ存在する行を表示しない
  #  -12: 両方のファイルに存在する行を表示
  comm -12 "$s3_files_list" "$local_files_list" >"$files_to_delete"

  local delete_count
  delete_count=$(wc -l <"$files_to_delete")
  info "==> Found $delete_count files to delete from S3"

  if [[ $delete_count -eq 0 ]]; then
    info "==> No files to delete"
    return 0
  fi

  warn "==> Deleting files (first 10):"
  head -10 "$files_to_delete" | sed 's/^/    /'
  if [[ $delete_count -gt 10 ]]; then
    warn "    ... and $((delete_count - 10)) more files"
  fi

  info "==> Deleting..."
  local deleted=0
  local failed=0

  while IFS= read -r file; do
    if aws s3 rm "s3://$bucket/$file" 2>/dev/null; then
      deleted=$((deleted + 1))
      if [[ $((deleted % 100)) -eq 0 ]]; then
        info "==> Progress: $deleted/$delete_count files deleted"
      fi
    else
      failed=$((failed + 1))
      warn "==> Failed to delete: $file"
    fi
  done <"$files_to_delete"

  info "==> Deletion complete: $deleted deleted, $failed failed"

  if [[ $failed -gt 0 ]]; then
    return 1
  fi
}

internal() {
  local dst_bucket_dir="$dst_root_dir/$bucket"
  local dst_tmp_sync_dir="$dst_root_dir/${timestamp}_${bucket}.tmp"
  mkdir -p "$dst_bucket_dir" "$dst_tmp_sync_dir"

  download_bucket
  copy_to_persistence

  trap end 0 1 2 3 15

  [[ $purge -ne 1 ]] && return
  purge_synced_files_from_s3
}

main() {
  initialize "$@"
  internal |& tee "$log_file"
}
main "$@"
